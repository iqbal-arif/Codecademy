1. Introduction

---

The ethical issues we have to make decisions about when incorporating generative AI into our work and personal lives can be incredibly subtle and sometimes very easy to overlook. Often, they fall into four different categories:

    1. Ethics of replacing human labor with automation

    Is it ethical to replace humans with AI? Is it sometimes ethical to replace humans with AI? This is important in the context of jobs like customer service and content creation jobs.

    2. Buying or selling AI-generated content

    Art, writing, music, and films all fall under content created in this category.

    3. Using AI for decision-making.

    Can we trust AI to make good and accurate decisions? Examples of this include resume screening and risk assessment algorithms (eg TSA clearance, applications for benefits, risk assessment tools used by the police, etc).

    4. Ethics of using autonomous machines

    This includes more than just autonomous driving and drones. This also includes facial recognition kiosks at airports or credit card machines.

It’s important to think about ethical considerations whenever using or interacting with AI.

2. Automation

---

Automation, the process of replacing human labor with machines or software, has been around for centuries. The first spinning mill, replacing a part of the yarn-making process done by hand for thousands of years, was introduced in the late 1700s.

Over the last several centuries, machines have replaced millions of jobs traditionally done by people. Some of these machines are programmed, and many follow simple procedures, repeating one or several parts of a process.

In more recent years, AI is becoming powerful enough to be integrated into machines that do work. Consider some of the following modern uses of AI:

    1. Manufacturing: Inspecting goods for quality control, predicting when equipment will begin to fail
    2. Logistics: Optimizing delivery routes, supply chain management
    3. Customer Service: Answering common questions and resolving simple issues
    Healthcare: Image analysis and diagnosis

In many cases, AI has been found to do them faster, cheaper, and more accurately than a person. However, in each case, AI is replacing a part of the work done by people and sometimes replacing people’s jobs altogether. Is this tradeoff acceptable?

Consider the following scenario:

    A large shipping company spends millions of dollars on a complex AI-powered logistics system. It is able to route each piece of the shipping process more effectively, reducing shipping times by a third. Human drivers and operators on the new routes are expected to follow the routes exactly, in the time predicted, or face disciplinary action. The company is able to let go of hundreds of full-time logistic experts and hires more engineers to continue to build new tech. Outperforming their competition, the new process causes dozens of small competitors to go out of business. These programs make the company the leader in its space.

Proponents of automation argue this is all a part of doing business and possibly beneficial to those involved. People displaced from jobs might receive training and be placed in different positions! (Unfortunately, this is often not the case.) Customers might receive benefits such as lower prices, lower wait times, or other improved outcomes.

Others might say that the use of proprietary AI systems is an unfair advantage. How is a small business supposed to compete and create its own system? Automation may lead to a loss of human skills and knowledge. As machines and software take over more tasks, fewer humans can perform these tasks, resulting in a loss of expertise and knowledge that is difficult to regain.

The use of AI and automation in business is a complex topic that has caused debate for hundreds of years. It has the potential to displace hundreds of millions of jobs but has also historically caused huge advancements in the way we, or at least some of us, live. Whether it is worth it or not is often left up to the individual to decide.

3. Art

---

In recent years, AI-powered products have been able to generate artwork that can match the skills of many digital artists. However, do these products match other artists too closely?

Consider the following scenario:

    Shelby is a digital artist specializing in alien planets and creatures. She has published a large catalog of copyrighted art on her website. The art she creates is fairly distinctive looking and recognizable.

    As a part of its training process, a new art tool, ArtMind scrapes the images off of Shelby’s website along with millions of other pieces collected from the internet.

    Frank uses ArtMind to generate a cover for his science fiction novel. He uses a prompt for an alien world similar to what is described in his novel. The book becomes a best seller.

    Shelby sees the novel in a bookstore and immediately recognizes the cover as being based on her art. She can even see some elements that are almost exact copies from pieces she’s made. Shelby decides to sue Frank for copyright infringement.

Stories like this are popping up within the digital art world. While AI tools are generating mostly new art, other people’s styles and elements are showing up in the art it creates. Who does this new art belong to, and who is responsible for ensuring there is no copyright infringement? The person providing the prompt? The company providing the tool? The artist of the training material?

AI-generated art can also take work away from human artists. Prompts can generate in seconds what might take an artist days to produce. Artists are finding their work taken away by machines that trained on their own art.

These machines allow widespread cheap access to the generation of custom digital art. Anyone is able to quickly produce beautiful images of whatever they can put into words. Do these benefits outweigh the costs?

4. Resume Screening and Discrimination

---

People tend to have biases that impact the decisions they make. With a machine in charge, will all decisions be fair?

Unfortunately, AI systems are subject to many of the same biases that humans are. The primary issue is that AIs are trained and programmed by biased humans.

One notable case of AI discrimination occurred in 2018 when Amazon developed an AI-powered hiring tool that used machine learning algorithms to screen resumes and identify the best candidates. The tool was designed to analyze resumes, score them, and identify the top candidates for open positions.

As a part of its training, the tool looked at resumes of people who were hired at Amazon over a ten-year period. Its job was to find the characteristics of people who get hired and recommend people like that to Amazon’s recruiters.

Tech companies often hire a lot more men than women. Because of this, the AI learned that a successful resume was most often one of a man.

As a result, the tool learned to prefer male candidates. It downgraded resumes that contained words or phrases more common to women, such as “women’s” or “female”. Candidates who went to all-female colleges were penalized as well.

Amazon discovered the bias during a test of the tool and subsequently scrapped the project.

This case illustrates the way in which historical bias in data can influence a new AI system. Similar problems have been found in loan systems, other resume screeners, and insurance programs.

AI will perpetuate bias and discrimination when the data used to train the algorithms is not diverse and representative of the population as a whole.

These biases can be difficult to detect because the AI systems do not explain their decision-making process, and they simply assign a score. This raises questions about how to ensure that AI systems are transparent and explainable, and how to ensure that they are not perpetuating bias and discrimination.

Creating AIs that avoid bias is an unsolved and difficult problem. Many of our datasets, such as the internet and literature, contain many harmful biases when broad patterns are applied to groups of people. Researchers continue to find ways to try and account for bias through practices such as using diverse datasets and thorough testing.

5. Autonomous Driving

---

We are living in a world where fully autonomous cars are being tested on the road. Not having to drive ourselves has the potential to be very convenient and safe, saving us time and attention. However, self-driving cars have their own set of complex ethical problems. Let’s explore some of the issues that self-driving car manufacturers and consumers need to consider.

The first issue is accountability: when something goes wrong in an autonomous system and someone gets hurt, who is held accountable? Autonomous cars rely on machine learning algorithms to make decisions and operate on the road. However, in the event of an accident, it can be difficult to assign responsibility to the car or its manufacturer. Do laws punishing dangerous drivers still apply to an artificial system?

Imagine the following situation:

    An autonomous car is driving down the road when suddenly, some pedestrians cross the road in front of it. The car’s sensors detect the pedestrians, but due to the speed of the car and the proximity of the pedestrians, it is unable to stop in time. The car’s algorithm must make a decision: should it continue straight and hit the group of pedestrians, or should it swerve to avoid them, potentially causing harm to the car’s passengers or other people in the vicinity?

Should the car prioritize the passengers, who own and pay for the vehicle? Or should it more heavily weigh the lives of pedestrians? Should the number of pedestrians matter? What if they are children? How should the AI choose, and how will it bear responsibility for that choice? These kinds of questions should be asked when designing systems that operate cars.

Those who drive for a living will also be affected by autonomous cars. Truck drivers, delivery drivers, taxi drivers, and more could see their careers disrupted by the use of autonomous vehicles. Like other forms of automation, self-driving cars could eliminate jobs and make smaller companies unable to compete.

Autonomous cars also raise concerns about privacy. Autonomous cars generate large amounts of data, including location data, driving behavior, and personal information about passengers. It is important to ensure that this data is collected, used, and shared in an ethical and responsible manner.

While self-driving cars may seem like a futuristic luxury, their development raise ethical considerations related to accountability, safety, and social impact. It is important for policymakers, industry leaders, and researchers to prioritize these ethical considerations and to work together to ensure that the deployment of autonomous cars is ethical, responsible, and beneficial to society.

6. Autonomous Weapons

---

Many technical advancements start from, or make their way to, the military. This is the case with artificial intelligence. Over the last few decades, the issue of autonomous weapons has developed.

Autonomous weapons use artificial intelligence to identify and engage targets without human control. Drones in combat today typically have a remote pilot controlling it. An autonomous version of this would have no human pilot, controlled solely by the AI. Completely autonomous weapons are in development but not confirmed to be in use yet (though there have been reports of their use).

Supporters of AI-powered weapon systems feel that they will be able to more accurately identify and kill a combatant than a human. They believe the use of these weapons could actually lower casualties of war.

However, having completely AI-controlled weapons is deeply controversial due to issues such as:

    Legality
    Accountability
    Safety

There are some legal issues involving the use of autonomous weapons. For example, it is a war crime to deploy autonomous weapons that cannot distinguish between civilians and combatants. An AI system would therefore need to be able to consistently identify who is a combatant. Given that modern wars often take place in urban areas, this is no easy task.

Consider the following scenario:

    A defense company has released a completely AI-powered drone for use in the military. In 99.9% of situations, it is able to correctly identify, engage, and kill combatants in urban environments without civilian casualties. This is far more accurate than using human soldiers. The US government deploys the drones in many different battles. However, after 100 battles, the drone misidentifies a target and kills a civilian.

Who is accountable when an autonomous weapon makes a mistake and kills a civilian? Is it the company, the government, or the engineer who wrote the code? Without a person making the decision to fire, accountability becomes more complicated. Who should be held responsible, and what should the consequences be?

The issue of accountability applies to wider AI as well. When third-party AI system makes a costly mistake, it is difficult to assign blame. It is also much more difficult to find out why the mistake happened. It is hard to examine an AI system and determine why a particular decision was made. These issues need to be carefully considered when creating AI systems, especially those that are responsible for human life.

It’s important to remember that AI aren’t human, and they make decisions based on their code and training - it can be difficult for them to understand nuance, human emotion, and international relations. It’s difficult to create laws to protect us from the dangers of AI.
